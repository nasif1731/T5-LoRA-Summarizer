{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \"transformers==4.38.2\" \"datasets==2.18.0\" \"accelerate==0.27.2\" \\\n                 \"huggingface_hub==0.20.3\" \"peft==0.9.0\" \"rouge_score\" \"nltk\" \\\n                 \"jax==0.4.23\" \"jaxlib==0.4.23\"\n\nimport os\nos._exit(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:19:34.229718Z","iopub.execute_input":"2025-11-02T12:19:34.230123Z","execution_failed":"2025-11-02T12:19:38.322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport nltk\nimport numpy as np\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import (\n    T5Tokenizer, \n    T5ForConditionalGeneration,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n\nMODEL_NAME = \"t5-small\"\nOUTPUT_DIR = \"/kaggle/working/t5-summarizer-lora\"\nDATA_PATH = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/\"\nTRAIN_FILE = os.path.join(DATA_PATH, \"train.csv\")\nVAL_FILE = os.path.join(DATA_PATH, \"validation.csv\")\nTEST_FILE = os.path.join(DATA_PATH, \"test.csv\")\nTRAIN_SAMPLES = 50000\nVAL_SAMPLES = 5000\nTEST_SAMPLES = 5000\nMAX_INPUT_LENGTH = 1024\nMAX_TARGET_LENGTH = 128\nTASK_PREFIX = \"summarize: \"\nnltk.download('punkt', quiet=True)\n\n\nprint(f\"Loading tokenizer for {MODEL_NAME}...\")\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nprint(\"Tokenizer loaded.\")\n\n\ndata_files = { \"train\": TRAIN_FILE, \"validation\": VAL_FILE, \"test\": TEST_FILE }\nfull_dataset = load_dataset(\"csv\", data_files=data_files)\nprint(\"Subsetting the dataset...\")\ntrain_dataset = full_dataset['train'].shuffle(seed=30).select(range(TRAIN_SAMPLES))\nval_dataset = full_dataset['validation'].shuffle(seed=30).select(range(VAL_SAMPLES))\ntest_dataset = full_dataset['test'].shuffle(seed=30).select(range(TEST_SAMPLES))\n\ndef preprocess_function(examples):\n    inputs = [TASK_PREFIX + str(doc) for doc in examples['article']]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n    labels = tokenizer(text_target=examples['highlights'], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"Applying tokenization to all subsets...\")\ntokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=os.cpu_count())\ntokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, num_proc=os.cpu_count())\ntokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, num_proc=os.cpu_count())\nprint(\"Data preprocessing complete.\")\n\n\nprint(f\"Loading base model: {MODEL_NAME}\")\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\nprint(\"Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\nmodel = get_peft_model(model, lora_config)\nprint(\"Model wrapped with LoRA.\")\nmodel.print_trainable_parameters() \n\nprint(\"Initializing Seq2Seq data collator...\")\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\nprint(\"Model and data collator loaded.\")\n\nrouge = evaluate.load(\"rouge\")\ndef compute_metrics(eval_preds):\n    predictions, labels = eval_preds\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels[labels == -100] = tokenizer.pad_token_id\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result = {key: value * 100 for key, value in result.items()}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return {k: round(v, 4) for k, v in result.items()}\nprint(\"ROGE compute_metrics function defined.\")\n\nif not torch.cuda.is_available():\n    print(\"WARNING: GPU not found. Training will be extremely slow.\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    \n    evaluation_strategy=\"no\", \n    save_strategy=\"no\",        \n    load_best_model_at_end=False,\n    warmup_steps=300,\n    weight_decay=0.01,\n    logging_dir=f\"{OUTPUT_DIR}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    report_to=\"none\",\n    fp16=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"--- Starting Model Fine-Tuning ---\")\ntorch.cuda.empty_cache() # Clear cache\ntrainer.train()\n\nprint(\"--- Training Complete ---\")\nprint(\"Model is trained.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:21:33.986294Z","iopub.execute_input":"2025-11-02T12:21:33.986978Z","iopub.status.idle":"2025-11-02T13:06:00.735845Z","shell.execute_reply.started":"2025-11-02T12:21:33.986949Z","shell.execute_reply":"2025-11-02T13:06:00.734954Z"}},"outputs":[{"name":"stderr","text":"2025-11-02 12:21:38.233613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762086098.257666   27686 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762086098.265026   27686 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading tokenizer for t5-small...\n","output_type":"stream"},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Tokenizer loaded.\nSubsetting the dataset...\nApplying tokenization to all subsets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ea089154664e42a407ebc0cef9c347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe148c093c1642e4bd41562442ad5abb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f1f3ebe8f3343a789e153ef1210a820"}},"metadata":{}},{"name":"stdout","text":"Data preprocessing complete.\nLoading base model: t5-small\nConfiguring LoRA...\nModel wrapped with LoRA.\ntrainable params: 294,912 || all params: 60,801,536 || trainable%: 0.4850403779272945\nInitializing Seq2Seq data collator...\nModel and data collator loaded.\nROGE compute_metrics function defined.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"--- Starting Model Fine-Tuning ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1562' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1562/1562 42:20, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>9.560200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>8.334900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.228600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.595800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.283300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.191900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.175400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.156300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.142000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.143600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.130600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.141600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.147100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.110000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.136600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"--- Training Complete ---\nModel is trained.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\n\nBASE_MODEL_NAME = \"t5-small\"\nLORA_MODEL_PATH = \"/kaggle/working/t5-summarizer-lora\"\n\nprint(f\"Loading base model: {BASE_MODEL_NAME}\")\nbase_model = T5ForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n\nprint(f\"Loading LoRA adapters from: {LORA_MODEL_PATH}\")\nmodel = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n\ntokenizer = T5Tokenizer.from_pretrained(LORA_MODEL_PATH)\n\nprint(\"Merging LoRA adapters into the base model...\")\nmodel = model.merge_and_unload()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nprint(f\"Fine-tuned model is loaded, merged, and on {device}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:08:10.466213Z","iopub.execute_input":"2025-11-02T13:08:10.467436Z","iopub.status.idle":"2025-11-02T13:08:11.365406Z","shell.execute_reply.started":"2025-11-02T13:08:10.467407Z","shell.execute_reply":"2025-11-02T13:08:11.364555Z"}},"outputs":[{"name":"stdout","text":"Loading base model: t5-small\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Loading LoRA adapters from: /kaggle/working/t5-summarizer-lora\nMerging LoRA adapters into the base model...\nFine-tuned model is loaded, merged, and on cuda.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import evaluate\nimport nltk\nfrom tqdm import tqdm \n\n\nrouge = evaluate.load(\"rouge\")\nNUM_SAMPLES_TO_EVAL = 50\n\ntry:\n    eval_samples = test_dataset.select(range(NUM_SAMPLES_TO_EVAL))\nexcept NameError:\n    print(\"Error: 'test_dataset' not in memory. Please re-run Phase 1, Cell 4.\")\n    raise\n\npredictions = []\nreferences = []\n\nprint(f\"--- Generating {NUM_SAMPLES_TO_EVAL} summaries for evaluation ---\")\n\nfor sample in tqdm(eval_samples):\n    article = sample['article']\n    reference_summary = sample['highlights']\n    prompt = TASK_PREFIX + article\n    input_ids = tokenizer(\n        prompt, \n        max_length=MAX_INPUT_LENGTH, \n        truncation=True, \n        padding=\"max_length\", \n        return_tensors=\"pt\"\n    ).input_ids.to(device) \n\n    \n    output_ids = model.generate(\n        input_ids=input_ids,\n        max_length=MAX_TARGET_LENGTH,\n        num_beams=4,  \n        early_stopping=True,\n        no_repeat_ngram_size=2 \n    )\n    \n    \n    predicted_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    predictions.append(predicted_summary)\n    references.append(reference_summary)\n\nprint(\"Generation complete.\")\n\nprint(\"\\n--- Calculating Final ROUGE Scores ---\")\n\n\ncleaned_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in predictions]\ncleaned_refs = [\"\\n\".join(nltk.sent_tokenize(ref.strip())) for ref in references]\n\nresult = rouge.compute(\n    predictions=cleaned_preds, \n    references=cleaned_refs, \n    use_stemmer=True\n)\nresult = {key: value * 100 for key, value in result.items()}\nresult = {k: round(v, 4) for k, v in result.items()}\n\nprint(result)\n\nprint(\"\\n--- Example Summaries (from Test Set) ---\")\nfor i in range(3):\n    print(f\"\\n===== EXAMPLE {i+1} =====\")\n    print(f\"\\nARTICLE:\\n{eval_samples[i]['article'][:700]}...\")\n    print(f\"\\nREFERENCE SUMMARY (Human):\\n{references[i]}\")\n    print(f\"\\nMODEL SUMMARY (Generated):\\n{predictions[i]}\")\n\nprint(\"\\n\\nTask 2 is complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:08:15.592390Z","iopub.execute_input":"2025-11-02T13:08:15.592684Z","iopub.status.idle":"2025-11-02T13:09:17.374555Z","shell.execute_reply.started":"2025-11-02T13:08:15.592660Z","shell.execute_reply":"2025-11-02T13:09:17.373876Z"}},"outputs":[{"name":"stdout","text":"--- Generating 50 summaries for evaluation ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50/50 [01:00<00:00,  1.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Generation complete.\n\n--- Calculating Final ROUGE Scores ---\n{'rouge1': 38.5214, 'rouge2': 16.8723, 'rougeL': 25.6611, 'rougeLsum': 34.6478}\n\n--- Example Summaries (from Test Set) ---\n\n===== EXAMPLE 1 =====\n\nARTICLE:\nUS officials are expected to stop prosecuting families of American hostages who communicate with kidnappers abroad or raise funds and pay ransoms. A National Counterterrorism Center advisory group, ordered by the White House, is expected to recommend what would mark a radical shift in US hostage policy, ABC news reported on Sunday. The NCTC interviewed families of hostages, including the parents of journalist James Foley, who was killed by Islamic State fighters. The family of a US contractor held by Al-Qaeda militants, Warren Weinstein (above), confirmed they paid a ransom in an attempt to secure his release which was reported in the amount of $250,000 . Foley's mother Diane said that offic...\n\nREFERENCE SUMMARY (Human):\nSenior official said there will be 'zero chance' that family of American held hostages overseas will face jail or prosecution for trying to free loved ones .\nA National Counterterrorism Center advisory group is set to recommend the move which would mark radical shift in US hostage policy .\nFamily of US contractor Warren Weinstein, who was taken by Al-Qaeda in Pakistan in 2011, paid a $250,000 ransom to try and secure his release .\nJournalist James Foley's mother said Obama administration told her it was illegal to raise a ransom to free her son .\nJames Foley was beheaded in August 2014 by Islamic State fighters .\n\nMODEL SUMMARY (Generated):\na national counterterrorism center advisory group, ordered by the white house, is expected to recommend what would mark radical shift in US hostage policy. the NCTC interviewed families of hostages, including the parents of journalist James Foley, who was killed by al-Qaeda fighters in 2011, and confirmed it had paid an ransom to try to secure his release, which was reported in the amount of $250,000. 'There will be absolutely zero chance of any family member of an American held hostaged overseas ever facing jail themselves, or even the\n\n===== EXAMPLE 2 =====\n\nARTICLE:\nThey say you should never forget your roots. Even more so in the case of this striking tree spotted in southern China. The banyan tree in Guangxi province does not stand on a single trunk, but a whole web of roots. It has captured the attention of locals, who say they have watched the whole thing sway in the wind due to its flimsy structure, reported the People's Daily Online. Scroll down for video . Fairy tale: The Banyan looks like enchanted trees found in children’s books . Uprooted: The original tree roots grew up against a wall, before it was moved to a nearby park . Originally the roots of the tree grew up against a wall, but when the wall was demolished the tree was carefully uprooted...\n\nREFERENCE SUMMARY (Human):\nRoots of the Banyan tree in southern China originally grew against a wall .\nWhen moved to its new home, the entangled roots became the trunk .\nThe structure is so lightweight that the entire tree sways in the breeze .\n\nMODEL SUMMARY (Generated):\nthe banyan tree in Guangxi province does not stand on a single trunk. it has captured the attention of locals, who say they have watched the whole thing sway in the wind due to its flimsy structure, according to the People's Daily Online.\n\n===== EXAMPLE 3 =====\n\nARTICLE:\nWith the helpless animals clutched tightly under his arms, this is the moment a suspected dog thief made off with two beloved family pets. Pekingese Marley and Mitzy had been left to play in the front garden by their owners Adele and Steven Worgan. But they were snatched when the couple weren’t looking – and neighbours believe they have caught the culprit on CCTV. Caught on camera: This is the suspected dog-napper seen on CCTV footage with what appears to be two animals under his arms . ‘We just want them back,’ said Mrs Worgan, 49. ‘It’s devastating. None of us have been able to sleep or eat since.’ Mr Worgan was inside the house in Doncaster at around midday when he heard a yelp from the g...\n\nREFERENCE SUMMARY (Human):\nPekingese dogs Marley and Mitzy were taken from garden in Doncaster .\nSuspected pet-napper caught on camera after the animals were snatched .\nTheft reported to the police and owners offer £1,000 reward for their return .\n\nMODEL SUMMARY (Generated):\npekingese Marley and Mitzy had been left to play in front garden by owners Adele and Steven Worgan. but they were snatched when they weren't looking - and neighbours believe they have caught the culprit on CCTV.\n\n\nTask 2 is complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n!zip -r t5_lora_model.zip /kaggle/working/t5-summarizer-lora\n\nprint(\"t5_lora_model.zip created successfully.\")\nprint(\"Please download it from the /kaggle/working/ directory in the file browser.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:27:15.178666Z","iopub.execute_input":"2025-11-02T13:27:15.179107Z","iopub.status.idle":"2025-11-02T13:27:15.500851Z","shell.execute_reply.started":"2025-11-02T13:27:15.179074Z","shell.execute_reply":"2025-11-02T13:27:15.499913Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/t5-summarizer-lora/ (stored 0%)\n  adding: kaggle/working/t5-summarizer-lora/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/t5-summarizer-lora/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/t5-summarizer-lora/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/t5-summarizer-lora/added_tokens.json (deflated 83%)\n  adding: kaggle/working/t5-summarizer-lora/adapter_config.json (deflated 51%)\n  adding: kaggle/working/t5-summarizer-lora/README.md (deflated 66%)\n  adding: kaggle/working/t5-summarizer-lora/spiece.model (deflated 48%)\n  adding: kaggle/working/t5-summarizer-lora/training_args.bin (deflated 51%)\nt5_lora_model.zip created successfully.\nPlease download it from the /kaggle/working/ directory in the file browser.\n","output_type":"stream"}],"execution_count":4}]}